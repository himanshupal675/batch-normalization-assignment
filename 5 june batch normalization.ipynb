{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0441c136-07d5-4f14-aecb-4cba7d273d18",
   "metadata": {},
   "source": [
    "## Answer 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cdccf2-501f-45c6-a8d2-e94addcab5d0",
   "metadata": {},
   "source": [
    "**Batch Normalization in Artificial Neural Networks:**\n",
    "\n",
    "**Concept:**\n",
    "Batch Normalization (BN) is a technique used in artificial neural networks to normalize the input of each layer during training. The normalization is performed across mini-batches, helping to stabilize and accelerate the training of deep neural networks.\n",
    "\n",
    "**Benefits of Using Batch Normalization:**\n",
    "\n",
    "1. **Stabilizing Learning:**\n",
    "   - BN addresses the issue of internal covariate shift, stabilizing the distribution of activations within each layer. This can lead to more consistent and faster learning during training.\n",
    "\n",
    "2. **Mitigating Vanishing/Exploding Gradients:**\n",
    "   - Batch normalization helps mitigate the vanishing and exploding gradient problems by normalizing the activations. This is particularly important in deep networks where gradients can become very small or large, affecting the convergence of the model.\n",
    "\n",
    "3. **Reducing Sensitivity to Initialization:**\n",
    "   - BN reduces the sensitivity of the network to the choice of weight initialization. This allows for the use of higher learning rates and contributes to more robust and reliable training.\n",
    "\n",
    "4. **Enabling Deeper Networks:**\n",
    "   - By normalizing activations within each layer, BN facilitates the training of deeper networks. This enables the construction of neural networks with more layers without encountering convergence issues.\n",
    "\n",
    "5. **Improving Generalization:**\n",
    "   - Batch normalization acts as a regularizer, reducing the need for other regularization techniques like dropout. It introduces a slight amount of noise during training, which can improve the generalization performance of the model.\n",
    "\n",
    "**Working Principle of Batch Normalization:**\n",
    "\n",
    "1. **Normalization Step:**\n",
    "   - For each mini-batch during training, BN normalizes the activations of each layer by subtracting the mean and dividing by the standard deviation of the mini-batch.\n",
    "   - The normalization step is mathematically expressed as:\n",
    "     \\[ \\hat{x} = \\frac{x - \\text{mean}(x)}{\\sqrt{\\text{var}(x) + \\epsilon}} \\]\n",
    "     where \\( \\hat{x} \\) is the normalized input, \\( x \\) is the input to the layer, \\( \\epsilon \\) is a small constant for numerical stability, and \\( \\text{mean}(x) \\) and \\( \\text{var}(x) \\) are the mean and variance of the mini-batch.\n",
    "\n",
    "2. **Learnable Parameters:**\n",
    "   - BN introduces learnable parameters for each channel (feature) in the form of scaling (\\( \\gamma \\)) and shifting (\\( \\beta \\)) factors.\n",
    "   - The normalized input \\( \\hat{x} \\) is then scaled by \\( \\gamma \\) and shifted by \\( \\beta \\) to obtain the final output of the normalization process.\n",
    "\n",
    "3. **Inference (Testing) Phase:**\n",
    "   - During inference, the mean and standard deviation used for normalization are often computed based on the entire training dataset or a moving average during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949fd06-64a4-490a-a141-e87b1fa174a9",
   "metadata": {},
   "source": [
    "## Answer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc27819-5dbc-4e0c-98cb-fbc147e81f5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define a simple feedforward neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist_dataset = MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "train_size = int(0.8 * len(mnist_dataset))\n",
    "val_size = len(mnist_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(mnist_dataset, [train_size, val_size])\n",
    "\n",
    "# Define data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "# Training function without batch normalization\n",
    "def train_model(model, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Create and train a model without batch normalization\n",
    "model_without_bn = SimpleNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_without_bn = optim.SGD(model_without_bn.parameters(), lr=0.01, momentum=0.9)\n",
    "train_model(model_without_bn, criterion, optimizer_without_bn)\n",
    "\n",
    "# Training function with batch normalization\n",
    "def train_model_with_bn(model, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Create and train a model with batch normalization\n",
    "class SimpleNNWithBN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNNWithBN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model_with_bn = SimpleNNWithBN().to(device)\n",
    "optimizer_with_bn = optim.SGD(model_with_bn.parameters(), lr=0.01, momentum=0.9)\n",
    "train_model_with_bn(model_with_bn, criterion, optimizer_with_bn)\n",
    "\n",
    "# Evaluate models on the validation set\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = correct / total\n",
    "    return accuracy\n",
    "\n",
    "# Evaluate models\n",
    "accuracy_without_bn = evaluate_model(model_without_bn, val_loader)\n",
    "accuracy_with_bn = evaluate_model(model_with_bn, val_loader)\n",
    "\n",
    "print(f\"Validation Accuracy without Batch Normalization: {accuracy_without_bn}\")\n",
    "print(f\"Validation Accuracy with Batch Normalization: {accuracy_with_bn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84405388-0a2e-4c26-a598-5b2d39558fbc",
   "metadata": {},
   "source": [
    "## Answer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b414726-f129-4374-ade5-3bea9fcf1f01",
   "metadata": {},
   "source": [
    "**Experimentation with Batch Sizes:**\n",
    "\n",
    "1. **Effect of Batch Size on Training Dynamics:**\n",
    "   - Train the neural network models with different batch sizes (e.g., 32, 64, 128).\n",
    "   - Observe how the training dynamics change, including the convergence speed and stability.\n",
    "   - Note any differences in the shape of the training and validation loss curves.\n",
    "\n",
    "2. **Effect on Model Performance:**\n",
    "   - Evaluate the models with different batch sizes on the validation set.\n",
    "   - Compare the final accuracy and loss of models trained with different batch sizes.\n",
    "   - Consider the trade-offs between faster convergence and potential overfitting.\n",
    "\n",
    "```python\n",
    "# Modify the batch sizes in the DataLoader initialization\n",
    "train_loader_batch_32 = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "train_loader_batch_64 = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "train_loader_batch_128 = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=4)\n",
    "\n",
    "# Train and evaluate models with different batch sizes\n",
    "model_batch_32 = SimpleNNWithBN().to(device)\n",
    "optimizer_batch_32 = optim.SGD(model_batch_32.parameters(), lr=0.01, momentum=0.9)\n",
    "train_model_with_bn(model_batch_32, criterion, optimizer_batch_32)\n",
    "\n",
    "model_batch_64 = SimpleNNWithBN().to(device)\n",
    "optimizer_batch_64 = optim.SGD(model_batch_64.parameters(), lr=0.01, momentum=0.9)\n",
    "train_model_with_bn(model_batch_64, criterion, optimizer_batch_64)\n",
    "\n",
    "model_batch_128 = SimpleNNWithBN().to(device)\n",
    "optimizer_batch_128 = optim.SGD(model_batch_128.parameters(), lr=0.01, momentum=0.9)\n",
    "train_model_with_bn(model_batch_128, criterion, optimizer_batch_128)\n",
    "\n",
    "# Evaluate models with different batch sizes\n",
    "accuracy_batch_32 = evaluate_model(model_batch_32, val_loader)\n",
    "accuracy_batch_64 = evaluate_model(model_batch_64, val_loader)\n",
    "accuracy_batch_128 = evaluate_model(model_batch_128, val_loader)\n",
    "\n",
    "print(f\"Validation Accuracy with Batch Size 32: {accuracy_batch_32}\")\n",
    "print(f\"Validation Accuracy with Batch Size 64: {accuracy_batch_64}\")\n",
    "print(f\"Validation Accuracy with Batch Size 128: {accuracy_batch_128}\")\n",
    "```\n",
    "\n",
    "**Advantages and Potential Limitations of Batch Normalization:**\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "1. **Stabilized Training:**\n",
    "   - Batch normalization helps stabilize and accelerate the training of neural networks by mitigating internal covariate shift.\n",
    "\n",
    "2. **Faster Convergence:**\n",
    "   - Networks with batch normalization often converge faster, allowing for shorter training times.\n",
    "\n",
    "3. **Reduced Sensitivity to Initialization:**\n",
    "   - Batch normalization reduces the sensitivity of the network to the choice of weight initialization, enabling the use of higher learning rates.\n",
    "\n",
    "4. **Improved Generalization:**\n",
    "   - Acts as a regularizer, reducing the need for other regularization techniques and improving generalization performance.\n",
    "\n",
    "5. **Facilitates Deeper Networks:**\n",
    "   - Enables the training of deeper networks by providing normalization within each layer.\n",
    "\n",
    "**Potential Limitations:**\n",
    "\n",
    "1. **Batch Size Sensitivity:**\n",
    "   - Batch normalization performance may vary with different batch sizes. Extremely small batch sizes may result in inaccurate statistics, affecting normalization.\n",
    "\n",
    "2. **Inference Overhead:**\n",
    "   - During inference, batch normalization may introduce some overhead due to the need to compute statistics for normalization. Techniques like running averages are often used to address this.\n",
    "\n",
    "3. **Dependency on Mini-Batch Statistics:**\n",
    "   - Batch normalization relies on mini-batch statistics, which may introduce noise during training. This noise can be beneficial for training but might not be desirable in all cases.\n",
    "\n",
    "4. **Not Suitable for Some Architectures:**\n",
    "   - While widely used in feedforward networks, batch normalization may not be suitable for recurrent neural networks (RNNs) and some specific architectures.\n",
    "\n",
    "5. **Complexity:**\n",
    "   - The introduction of additional learnable parameters and normalization steps increases the complexity of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5c00e-3464-4f3c-abe3-d5a0e0d6705a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
